{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0c9ff2",
   "metadata": {},
   "source": [
    "# <font face=\"gotham\" color=\"Green\">  Prototyping with Deep Learning  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122c503",
   "metadata": {},
   "source": [
    "# <font face=\"gotham\" color=\"Brown\">  Getting started with  </font>\n",
    "\n",
    "\n",
    "![PyTorch](imgs/pytorch.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf95b1",
   "metadata": {},
   "source": [
    "Official resources:\n",
    "* [Deep Learning with PyTorch: a 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* [PyTorch documentation](https://pytorch.org/docs/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587776dc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Basic concepts\n",
    "\n",
    "Tensors\n",
    "--------------------------------------------\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model‚Äôs parameters.\n",
    "\n",
    "Tensors are similar to NumPy‚Äôs ndarrays, except that tensors can run on\n",
    "GPUs or other specialized hardware to accelerate computing. If you‚Äôre familiar with ndarrays, you‚Äôll\n",
    "be right at home with the Tensor API. If not, follow along in this quick\n",
    "API walkthrough.\n",
    "\n",
    "> Tensors are IMMUTABLE. Just like python tuples and sets you can NOT update the contents of a tensor. You can only create a new tensor. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063c924",
   "metadata": {},
   "source": [
    "Install [PyTorch](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f88f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33970c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2fae5",
   "metadata": {},
   "source": [
    "Tensor Initialization\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a02d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e15ebc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd83ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f52fe3",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d9a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coverts list to np.array\n",
    "np_array = np.array(data)\n",
    "#convert np.array to tensor\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef149a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd310b6",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14a456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9902, 0.8660],\n",
      "        [0.2200, 0.2702]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) \n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) \n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98237297",
   "metadata": {},
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723f72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.2306, 0.9226, 0.0769],\n",
      "        [0.2179, 0.1741, 0.4137]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6b4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c39e904",
   "metadata": {},
   "source": [
    "#### Tensor Attributes\n",
    "\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37053ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b919343",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92511a",
   "metadata": {},
   "source": [
    "#### Tensor Operations\n",
    "\n",
    "\n",
    "Over 100 tensor operations, including transposing, indexing, slicing,\n",
    "mathematical operations, linear algebra, random sampling, and more are\n",
    "comprehensively described\n",
    "[here](https://pytorch.org/docs/stable/torch.html)\n",
    "\n",
    "Each of them can be run on the GPU (at typically higher speeds than on a\n",
    "CPU). If you‚Äôre using Colab, allocate a GPU by going to Edit > Notebook\n",
    "Settings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff560db",
   "metadata": {},
   "source": [
    "**Multiplying tensors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bf502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[0.0101, 0.1809, 0.0163, 0.9717],\n",
      "        [0.3035, 0.0636, 0.0080, 0.1841],\n",
      "        [0.0120, 0.9534, 0.4247, 0.4839]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[0.0101, 0.1809, 0.0163, 0.9717],\n",
      "        [0.3035, 0.0636, 0.0080, 0.1841],\n",
      "        [0.0120, 0.9534, 0.4247, 0.4839]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0252948",
   "metadata": {},
   "source": [
    "This computes the matrix multiplication between two tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa0d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[1.1790, 0.5970, 1.1951],\n",
      "        [0.5970, 0.5591, 0.6633],\n",
      "        [1.1951, 0.6633, 1.8740]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[1.1790, 0.5970, 1.1951],\n",
      "        [0.5970, 0.5591, 0.6633],\n",
      "        [1.1951, 0.6633, 1.8740]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9187bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810213c1",
   "metadata": {},
   "source": [
    "#### Tensor to NumPy array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f28d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1957f",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2759ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f803e",
   "metadata": {},
   "source": [
    "NumPy array to Tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2125acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fa952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d949d7a1",
   "metadata": {},
   "source": [
    " # 2. Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9350c",
   "metadata": {},
   "source": [
    "## 2.1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfeb32",
   "metadata": {},
   "source": [
    "![PyTorch](imgs/data.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119a954",
   "metadata": {},
   "source": [
    "**Handling data** ( i.e. steps of  acquiring, preprocessing and how you will iterate over your data) consumes majority of your time while working on ML projects. In this tutorial we will learn a popular dataset called MNIST and use a package called [torchvision](https://pytorch.org/vision/stable/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fcb3c",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset has 70k small grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b129ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec2d793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f840dd",
   "metadata": {},
   "source": [
    "![MNIST](imgs/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91307fec",
   "metadata": {},
   "source": [
    "#### Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    " '''Eventhough we are grabbing the data from Pytorch,\n",
    "    it's not in tensor format so we use the transform \n",
    "    method to convert it to tensors.'''\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d1181",
   "metadata": {},
   "source": [
    "***Loading data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f9eaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True) \n",
    "test_set = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5772c31",
   "metadata": {},
   "source": [
    "batch_size is how many datapoints we want to pass at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4406694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([3, 9, 8, 8, 6, 2, 6, 0, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "#Iterate over the trainset\n",
    "#Output 10 examples of handwritten digits(x) and 10 tensors of the actual outpputs(y)\n",
    "for data in train_set:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b21aa",
   "metadata": {},
   "source": [
    "Each iteration will contain a batch of 10 elements (that was the batch size we chose), and 10 classes. Let's just look at one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26f64131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "X, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee49b4",
   "metadata": {},
   "source": [
    "* X is our input data. The features. The thing we want to predict.\n",
    "* y is our label. The classification. The thing we hope the neural network can learn to predict.\n",
    "\n",
    "We can see this by doing.\n",
    "\n",
    "```data[0]``` is a bunch of features for things and ```data[1]``` is all the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5446189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae17b8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 8, 8, 6, 2, 6, 0, 3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5d85e",
   "metadata": {},
   "source": [
    "As you can see, ```data[1]``` is just a bunch of labels. So, since ```data[1][0]``` is a 7, we can expect ```data[0][0]``` to be an image of a 7. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53c2920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.imshow(data[0][0])\n",
    "#throws shape error \n",
    "plt.imshow(data[0][0].view(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6213",
   "metadata": {},
   "source": [
    "To learn Visualization with Python check out [matplotlib](https://matplotlib.org/stable/tutorials/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37a7bedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5916d4c",
   "metadata": {},
   "source": [
    "Now we have our data downloaded and loaded to memory as train_set and test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82f80f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07cbc640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9eb92",
   "metadata": {},
   "source": [
    "## 2.2. Defining a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c43a3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # OOP\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfe862",
   "metadata": {},
   "source": [
    "The ```torch.nn``` import gives us access to some helpful neural network things, such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc)\n",
    "\n",
    "The ```torch.nn.functional```  specifically gives us access to some handy functions that we might not want to write ourselves. such as relu or \"rectified linear\" activation function. Instead of writing all of the code for these things this allows us to just import and use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccc6be",
   "metadata": {},
   "source": [
    "To build our model, we're going to create a class. We'll call this class net and this net will inhereit from the nn.Module class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c23d0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)  #784 is 28*28 flattened image Input output is 64 can be any\n",
    "        self.fc2 = nn.Linear(64, 64) #Hidden layers\n",
    "        self.fc3 = nn.Linear(64 , 64)\n",
    "        self.fc4 = nn.Linear(64, 10) #There are 10 classes\n",
    "        \n",
    "        #We defined our NN but no way to feed in data\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) #  relu, activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = self.fc4(x) #No activation\n",
    "        return output\n",
    "        \n",
    "\n",
    "net = Net()\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef3f9a",
   "metadata": {},
   "source": [
    "First layer takes in 28x28, because our images are 28x28 images of hand-drawn digits. A basic neural network is going to expect to have a flattened array, so not a 28x28, but instead a 1x784.\n",
    "\n",
    "We used relu(rectified linear unint) as activation function. \n",
    "\n",
    "Basically, these activation functions are keeping our data scaled between 0 and 1.\n",
    "\n",
    "Finally, for the output layer, we used softmax. Softmax makes sense to use for a multi-class problem, where each thing can only be one class or the other. This means the outputs themselves are a confidence score, adding up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716f82d",
   "metadata": {},
   "source": [
    "There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:\n",
    "- The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "- The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "- The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "Moreover, the number of neurons and number layers required for the hidden layer also depends upon training cases, the complexity of, data that is to be learned, and the type of activation functions used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49292c1",
   "metadata": {},
   "source": [
    "## 2.3. Defining a Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53b3097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # Adam:Adaptive momentm ,  lr: learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f63edf",
   "metadata": {},
   "source": [
    "* Our ```loss_function``` is what calculates \"how far off\" our classifications are from reality.\n",
    "It is a measurement of how far off the neural network is from the targeted output.\n",
    "\n",
    "\n",
    "![Softmax](imgs/softmax.png)\n",
    "\n",
    "\n",
    "\n",
    "In pytorch the ```nn.CrossEntropyLoss()``` already applies softmax so no need to add softmax at the last layer.\n",
    "\n",
    "![Sigmoid](imgs/Sigmoid.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df41233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)  #784 is 28*28 flattened image Input output is 64 can be any\n",
    "        self.fc2 = nn.Linear(64, 64) #Hidden layers\n",
    "        self.fc3 = nn.Linear(64 , 64)\n",
    "        self.fc4 = nn.Linear(64, 1) #one out put Yes or No\n",
    "        \n",
    "        #\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) #  relu, activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        output = self.fc4(x) \n",
    "        #Sigmoid at the end\n",
    "        y_pred = torch.sigmoid(output)\n",
    "        return ypred\n",
    "        #F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52044395",
   "metadata": {},
   "source": [
    "* ```optimizer``` adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data.\n",
    "\n",
    "* The learning rate dictates the magnitude of changes that the optimizer can make at a time. Thus, the larger the LR, the quicker the model can learn, but also you might find that the steps you allow the optimizer to make are actually too big and the optimizer gets stuck bouncing around rather than improving. Too small, and the model can take much longer to learn as well as also possibly getting stuck. Indeed a too small value will require a very large number of epochs to converge while the algorithm might not converged by setting a too large value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef389580",
   "metadata": {},
   "source": [
    "![LR](https://deeplearningmath.org/images/learning_rate_choice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961eea0",
   "metadata": {},
   "source": [
    "Moreover, it is not recommended to use a constant learning rate. Indeed, even if a large value can help the algorithm to arrive quickly to a good solution, then it might oscillate around this state for a long time or diverge if the learning rate is maintained. A solution is to allow the learning rate to decay over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f8b3d",
   "metadata": {},
   "source": [
    "> For simpler tasks, a learning rate of 0.001 usually is more than fine. For more complex tasks, you will see a learning rate with what's called a decay. Basically you start the learning rate at something like 0.001, or 0.01...etc, and then over time, that learning rate gets smaller and smaller. The idea being you can initially train fast, and slowly take smaller steps, hopefully getthing the best of both worlds.\n",
    "\n",
    "A common approach is to half the learning rate every 5 epochs, or by 0.1 every 20 epochs. A proposed heuristic is to track the validation error while training with a fixed learning rate, and if the validation error stops improving then reduce the learning rate by a constant (e.g. 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba46a9",
   "metadata": {},
   "source": [
    "## 2.4. Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f28ab5",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "\n",
    "![Epoch](https://miro.medium.com/max/1024/1*cDhZ56QNC5mrl6kjE0C2JA.png)\n",
    "\n",
    "In deep learning an epoch is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) which is defined before training a model. In other words, one epoch is when an entire dataset is passed both forward and backward through the neural network only once.\n",
    "\n",
    "The reason why we have to split the training step by epochs is decrease the amount of data we feed to the computer at once. So, we divide it in several smaller batches. \n",
    "\n",
    "We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. But since we are using a limited dataset we can do it in an iterative process. A batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b866f",
   "metadata": {},
   "source": [
    "* Too few epochs, and your model wont learn everything it could have.\n",
    "\n",
    "* Too many epochs and your model will over fit to your in-sample data (basically memorize the in-sample data, and perform poorly on out of sample data).\n",
    "\n",
    "Let's go with 3 epochs for now. So we will loop over epochs, and each epoch will loop over our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "105ce981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [3000/6000], Loss: 0.0278\n",
      "Epoch [1/3], Step [6000/6000], Loss: 0.0036\n",
      "Epoch [2/3], Step [3000/6000], Loss: 0.2456\n",
      "Epoch [2/3], Step [6000/6000], Loss: 0.0259\n",
      "Epoch [3/3], Step [3000/6000], Loss: 0.0332\n",
      "Epoch [3/3], Step [6000/6000], Loss: 0.0260\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = len(train_set)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs): #3 full passes over the data\n",
    "    for i, (images, labels) in enumerate(train_set):  \n",
    "        # origin shape: [10, 1, 28, 28]\n",
    "        # resized: [10, 784]\n",
    "        images = images.reshape(-1, 28*28) #with -1 Pytorch will automatically identify the first dimentions\n",
    "        labels = labels\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(images) # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = loss_function(outputs, labels) # calculate and grab the loss value\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step() # attempt to optimize weights to account for loss/gradients\n",
    "        \n",
    "        if (i+1) % 3000 == 0: #Every 3000 steps prinyt some info \n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebb7a4",
   "metadata": {},
   "source": [
    "Once we pass data through our neural network, getting an output, we can compare that output to the desired output. With this, we can compute the gradients for each parameter, which our optimizer (Adam, SGD...etc) uses as information for updating weights.\n",
    "\n",
    "\n",
    "This is why it's important to do a ```net.zero_grad()``` for every step, otherwise these gradients will add up for every pass, and then we'll be re-optimizing for previous gradients that we already optimized for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a57af",
   "metadata": {},
   "source": [
    "So, for each epoch, and for each batch in our dataset, what do we do?\n",
    "\n",
    "* Grab the features (X) and labels (y) from current batch\n",
    "* Zero the gradients (net.zero_grad)\n",
    "* Pass the data through the network\n",
    "* Calculate the loss\n",
    "* Adjust weights in the network with the hopes of decreasing loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403cf0f",
   "metadata": {},
   "source": [
    "## 2.5. Test the network on the test data\n",
    "\n",
    "As we iterate, we get loss, which is an important metric, but we care about accuracy. So, how did we do? To test this, all we need to do is iterate over our test set, measuring for correctness by comparing output to target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6332d8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 95.93 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "# This is out of sample data and we just want to know how good is the network \n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_set:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        labels = labels\n",
    "        outputs = net(images)\n",
    "        \n",
    "        for idx, i in enumerate(outputs):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == labels[idx]:\n",
    "                n_correct += 1\n",
    "            n_samples += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5e82447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random testing images\n",
    "dataiter = iter(test_set)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f731d4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[2].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "797e96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(images[2].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf1fd8",
   "metadata": {},
   "source": [
    "## 2.6.  Saving and Loading the model\n",
    "\n",
    "Model training usually takes a lot of time, so once the model is trained it is smart to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a83d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'Models/torch_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "828dad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('Models/torch_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22465b9",
   "metadata": {},
   "source": [
    "## üèÅ 6. Conclusion\n",
    "\n",
    "Now, you know:\n",
    "\n",
    "1. a popular deep learning framework: PyTorch,\n",
    "2. the basic building blocks of deep learning,\n",
    "3. how to load data and define a Neural Network, \n",
    "4. how to train and test a Neural Network using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6db06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
